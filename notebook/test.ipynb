{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "base_url = \"https://baike.baidu.com\"\n",
    "his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\n"
     ]
    }
   ],
   "source": [
    "url = base_url + his[-1]\n",
    "\n",
    "html = urlopen(url).read().decode('utf-8')\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "print(soup.find('h1').get_text(), '    url: ', his[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711', '/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E']\n"
     ]
    }
   ],
   "source": [
    "# find valid urls\n",
    "sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})\n",
    "if len(sub_urls) != 0:\n",
    "    his.append(random.sample(sub_urls, 1)[0]['href'])\n",
    "else:\n",
    "    # no valid sub link found\n",
    "    his.pop()\n",
    "print(his)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 网络爬虫  url: /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\n",
      "1 www  url: /item/%E4%B8%87%E7%BB%B4%E7%BD%91\n",
      "2 www  url: /item/%E4%B8%87%E7%BB%B4%E7%BD%91\n",
      "3 俄罗斯  url: /item/%E4%BF%84%E7%BD%97%E6%96%AF\n",
      "4 国际歌  url: /item/%E5%9B%BD%E9%99%85%E6%AD%8C\n",
      "5 杨起  url: /item/%E6%9D%A8%E8%B5%B7\n",
      "6 刘光鼎  url: /item/%E5%88%98%E5%85%89%E9%BC%8E\n",
      "7 刘本钊  url: /item/%E5%88%98%E6%9C%AC%E9%92%8A\n",
      "8 醉八仙  url: /item/%E9%85%92%E4%B8%AD%E5%85%AB%E4%BB%99\n",
      "9 焦遂  url: /item/%E7%84%A6%E9%81%82\n",
      "10 甘泽谣  url: /item/%E7%94%98%E6%B3%BD%E8%B0%A3\n",
      "11 太平广记  url: /item/%E5%A4%AA%E5%B9%B3%E5%B9%BF%E8%AE%B0\n",
      "12 相一  url: /item/%E7%9B%B8%E4%B8%80\n",
      "地址为空\n",
      "13 太平广记  url: /item/%E5%A4%AA%E5%B9%B3%E5%B9%BF%E8%AE%B0\n",
      "14 赵光义  url: /item/%E8%B5%B5%E7%82%85\n",
      "15 高梁河  url: /item/%E9%AB%98%E7%B2%B1%E6%B2%B3\n",
      "16 高梁桥  url: /item/%E9%AB%98%E6%A2%81%E6%A1%A5\n",
      "17 北京  url: /item/%E5%8C%97%E4%BA%AC%E5%B8%82\n",
      "18 羊头马  url: /item/%E7%BE%8A%E5%A4%B4%E9%A9%AC\n",
      "19 青云阁  url: /item/%E9%9D%92%E4%BA%91%E9%98%81\n",
      "地址为空\n",
      "20 羊头马  url: /item/%E7%BE%8A%E5%A4%B4%E9%A9%AC\n",
      "21 道光  url: /item/%E9%81%93%E5%85%89\n",
      "22 爱新觉罗·载铨  url: /item/%E8%BD%BD%E9%93%A8\n",
      "23 许诵恒  url: /item/%E8%AE%B8%E8%AF%B5%E6%81%92\n",
      "24 爱新觉罗·奕詝  url: /item/%E5%92%B8%E4%B8%B0\n",
      "25 第二次鸦片战争  url: /item/%E7%AC%AC%E4%BA%8C%E6%AC%A1%E9%B8%A6%E7%89%87%E6%88%98%E4%BA%89\n",
      "26 望厦条约  url: /item/%E6%9C%9B%E5%8E%A6%E6%9D%A1%E7%BA%A6\n",
      "27 道光  url: /item/%E9%81%93%E5%85%89\n",
      "28 爱新觉罗·旻宁  url: /item/%E7%88%B1%E6%96%B0%E8%A7%89%E7%BD%97%C2%B7%E6%97%BB%E5%AE%81\n",
      "29 大运河  url: /item/%E5%A4%A7%E8%BF%90%E6%B2%B3\n",
      "30 积水潭  url: /item/%E7%A7%AF%E6%B0%B4%E6%BD%AD\n",
      "31 内务府  url: /item/%E5%86%85%E5%8A%A1%E5%BA%9C\n",
      "32 马群街道  url: /item/%E9%A9%AC%E7%BE%A4\n",
      "33 南京禄口国际机场  url: /item/%E5%8D%97%E4%BA%AC%E7%A6%84%E5%8F%A3%E5%9B%BD%E9%99%85%E6%9C%BA%E5%9C%BA\n",
      "34 南京都市圈  url: /item/%E5%8D%97%E4%BA%AC%E9%83%BD%E5%B8%82%E5%9C%88\n",
      "35 南京八卦洲长江大桥  url: /item/%E5%8D%97%E4%BA%AC%E9%95%BF%E6%B1%9F%E4%BA%8C%E6%A1%A5\n",
      "地址为空\n",
      "36 南京都市圈  url: /item/%E5%8D%97%E4%BA%AC%E9%83%BD%E5%B8%82%E5%9C%88\n",
      "37 滁州  url: /item/%E6%BB%81%E5%B7%9E\n",
      "38 许继伟  url: /item/%E8%AE%B8%E7%BB%A7%E4%BC%9F\n",
      "39 黄山  url: /item/%E9%BB%84%E5%B1%B1%E5%B8%82\n",
      "40 徽语  url: /item/%E5%BE%BD%E8%AF%AD\n",
      "41 淳安县  url: /item/%E6%B7%B3%E5%AE%89%E5%8E%BF\n",
      "42 黄铁矿  url: /item/%E9%BB%84%E9%93%81%E7%9F%BF\n",
      "43 热电性  url: /item/%E7%83%AD%E7%94%B5%E6%80%A7\n",
      "44 热电材料  url: /item/%E7%83%AD%E7%94%B5%E6%9D%90%E6%96%99\n",
      "45 功率因子  url: /item/%E5%8A%9F%E7%8E%87%E5%9B%A0%E5%AD%90\n",
      "46 正弦电路  url: /item/%E6%AD%A3%E5%BC%A6%E7%94%B5%E8%B7%AF\n",
      "地址为空\n",
      "47 功率因子  url: /item/%E5%8A%9F%E7%8E%87%E5%9B%A0%E5%AD%90\n",
      "48 功率因数  url: /item/%E5%8A%9F%E7%8E%87%E5%9B%A0%E6%95%B0\n",
      "49 发电机  url: /item/%E5%8F%91%E7%94%B5%E6%9C%BA\n",
      "50 负荷  url: /item/%E8%B4%9F%E8%8D%B7\n",
      "地址为空\n",
      "51 发电机  url: /item/%E5%8F%91%E7%94%B5%E6%9C%BA\n",
      "52 抽水蓄能电站  url: /item/%E6%8A%BD%E6%B0%B4%E8%93%84%E8%83%BD%E7%94%B5%E7%AB%99\n",
      "53 发电厂  url: /item/%E5%8F%91%E7%94%B5%E5%8E%82\n",
      "54 水电  url: /item/%E6%B0%B4%E7%94%B5\n",
      "55 火电  url: /item/%E7%81%AB%E7%94%B5\n",
      "56 汽轮机  url: /item/%E6%B1%BD%E8%BD%AE%E6%9C%BA\n",
      "57 真空泵  url: /item/%E7%9C%9F%E7%A9%BA%E6%B3%B5\n",
      "58 润滑油  url: /item/%E6%B6%A6%E6%BB%91%E6%B2%B9\n",
      "59 抗氧剂  url: /item/%E6%8A%97%E6%B0%A7%E5%89%82\n",
      "60 化妆品  url: /item/%E5%8C%96%E5%A6%86%E5%93%81\n",
      "61 美白  url: /item/%E7%BE%8E%E7%99%BD\n",
      "62 护手霜  url: /item/%E6%8A%A4%E6%89%8B%E9%9C%9C\n",
      "63 面霜  url: /item/%E9%9D%A2%E9%9C%9C\n",
      "64 敏感性皮肤  url: /item/%E6%95%8F%E6%84%9F%E7%9A%AE%E8%82%A4\n",
      "地址为空\n",
      "65 面霜  url: /item/%E9%9D%A2%E9%9C%9C\n",
      "66 丙三醇  url: /item/%E7%94%98%E6%B2%B9\n",
      "67 丙烯  url: /item/%E4%B8%99%E7%83%AF\n",
      "68 通风系统  url: /item/%E9%80%9A%E9%A3%8E%E7%B3%BB%E7%BB%9F\n",
      "地址为空\n",
      "69 丙烯  url: /item/%E4%B8%99%E7%83%AF\n",
      "70 通风系统  url: /item/%E9%80%9A%E9%A3%8E%E7%B3%BB%E7%BB%9F\n",
      "地址为空\n",
      "71 丙烯  url: /item/%E4%B8%99%E7%83%AF\n",
      "72 静电  url: /item/%E9%9D%99%E7%94%B5\n",
      "73 海水淡化  url: /item/%E6%B7%A1%E5%8C%96%E6%B5%B7%E6%B0%B4\n",
      "74 大港区  url: /item/%E5%A4%A9%E6%B4%A5%E5%A4%A7%E6%B8%AF\n",
      "75 永定河  url: /item/%E6%B0%B8%E5%AE%9A%E6%B2%B3\n",
      "76 义项  url: /item/%E4%B9%89%E9%A1%B9\n",
      "77 少将  url: /item/%E5%B0%91%E5%B0%86\n",
      "78 中将  url: /item/%E4%B8%AD%E5%B0%86\n",
      "79 少校  url: /item/%E5%B0%91%E6%A0%A1\n",
      "80 百度百科：多义词  url: /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E5%A4%9A%E4%B9%89%E8%AF%8D\n",
      "81 赵氏孤儿大报仇  url: /item/%E8%B5%B5%E6%B0%8F%E5%AD%A4%E5%84%BF\n",
      "82 百度百科：多义词  url: /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E5%A4%9A%E4%B9%89%E8%AF%8D\n",
      "83 赵氏孤儿大报仇  url: /item/%E8%B5%B5%E6%B0%8F%E5%AD%A4%E5%84%BF\n",
      "84 百度百科：多义词  url: /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E5%A4%9A%E4%B9%89%E8%AF%8D\n",
      "85 赵氏孤儿大报仇  url: /item/%E8%B5%B5%E6%B0%8F%E5%AD%A4%E5%84%BF\n",
      "86 义项  url: /item/%E4%B9%89%E9%A1%B9\n",
      "87 钢铁是怎样炼成的  url: /item/%E9%92%A2%E9%93%81%E6%98%AF%E6%80%8E%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84\n",
      "88 苏维埃  url: /item/%E8%8B%8F%E7%BB%B4%E5%9F%83\n",
      "89 全俄苏维埃第二次代表大会  url: /item/%E5%85%A8%E4%BF%84%E8%8B%8F%E7%BB%B4%E5%9F%83%E7%AC%AC%E4%BA%8C%E6%AC%A1%E4%BB%A3%E8%A1%A8%E5%A4%A7%E4%BC%9A\n",
      "地址为空\n",
      "90 苏维埃  url: /item/%E8%8B%8F%E7%BB%B4%E5%9F%83\n",
      "91 苏德战争  url: /item/%E5%8D%AB%E5%9B%BD%E6%88%98%E4%BA%89\n",
      "92 格奥尔吉·康斯坦丁诺维奇·朱可夫  url: /item/%E6%9C%B1%E5%8F%AF%E5%A4%AB\n",
      "93 斯莫尔尼宫  url: /item/%E6%96%AF%E8%8E%AB%E5%B0%94%E5%B0%BC%E5%AE%AB\n",
      "94 大隐修院  url: /item/%E4%BF%AE%E9%81%93%E9%99%A2\n",
      "地址为空\n",
      "95 斯莫尔尼宫  url: /item/%E6%96%AF%E8%8E%AB%E5%B0%94%E5%B0%BC%E5%AE%AB\n",
      "96 布尔什维克党  url: /item/%E5%B8%83%E5%B0%94%E4%BB%80%E7%BB%B4%E5%85%8B\n",
      "97 哈尔科夫  url: /item/%E5%93%88%E5%B0%94%E7%A7%91%E5%A4%AB\n",
      "98 犹太族  url: /item/%E7%8A%B9%E5%A4%AA%E6%97%8F\n",
      "地址为空\n",
      "99 哈尔科夫  url: /item/%E5%93%88%E5%B0%94%E7%A7%91%E5%A4%AB\n"
     ]
    }
   ],
   "source": [
    "his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]\n",
    "for i in range(100):\n",
    "    url = base_url+his[-1]\n",
    "    html = urlopen(url).read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html,features=\"lxml\")\n",
    "    print(i,soup.find(\"h1\").get_text(),\" url:\",his[-1])\n",
    "\n",
    "    sub_urls = soup.find_all(\"a\",{\"target\":\"_blank\",\"href\":re.compile(\"^/item/(%.{2})+$\")})\n",
    "    if len(sub_urls)!=0:\n",
    "        his.append(random.sample(sub_urls,1)[0]['href'])\n",
    "    else:\n",
    "        his.pop()\n",
    "        print(\"地址为空\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wappass.baidu.com/static/captcha/tuxing.html?&ak=c27bbc89afca0463650ac9bde68ebe06&backurl=https%3A%2F%2Fwww.baidu.com%2Fs%3Fwd%3D%25E8%258E%25AB%25E7%2583%25A6Python&logid=7060467936256501563&signature=6f353564accdaf5415101a93a38bcba0&timestamp=1582580136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import webbrowser\n",
    "param = {\"wd\": \"莫烦Python\"}  # 搜索的信息\n",
    "r = requests.get('http://www.baidu.com/s', params=param)\n",
    "print(r.url)\n",
    "webbrowser.open(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wappass.baidu.com/static/captcha/tuxing.html?&ak=c27bbc89afca0463650ac9bde68ebe06&backurl=https%3A%2F%2Fwww.baidu.com%2Fs%3Fwd%3D%25E8%258E%25AB%25E7%2583%25A6Python&logid=8016863638961865832&signature=2701131a9d6c26316a58844e589bd78b&timestamp=1582585070\n"
     ]
    }
   ],
   "source": [
    "param = {\"wd\": \"莫烦Python\"}  # 搜索的信息\n",
    "r = requests.get('http://www.baidu.com/s', params=param)\n",
    "print(r.url)\n",
    "# webbrowser.open(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there,  !\n"
     ]
    }
   ],
   "source": [
    "data = {'firstname': 'hh', 'lastname': 'xx'}\n",
    "r = requests.post('http://pythonscraping.com/files/processing.php', data=data)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploads/logo.png\n",
      "The file logo.png has been uploaded.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "file = {'uploadFile': open('./logo.png', 'rb')}\n",
    "r = requests.post('http://pythonscraping.com/pages/files/processing2.php', files=file)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    666\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    420\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 远程主机强迫关闭了一个现有的连接。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    720\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    666\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    420\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f0a19d802ca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# sending get request and saving the response as response object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPARAMS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# extracting data in json format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\spider\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "# importing the requests library \n",
    "import requests \n",
    "  \n",
    "# api-endpoint \n",
    "URL = \"http://maps.googleapis.com/maps/api/geocode/json\"\n",
    "  \n",
    "# location given here \n",
    "location = \"delhi technological university\"\n",
    "  \n",
    "# defining a params dict for the parameters to be sent to the API \n",
    "PARAMS = {'address':location} \n",
    "  \n",
    "# sending get request and saving the response as response object \n",
    "r = requests.get(url = URL, params = PARAMS) \n",
    "  \n",
    "# extracting data in json format \n",
    "data = r.json() \n",
    "  \n",
    "  \n",
    "# extracting latitude, longitude and formatted address  \n",
    "# of the first matching location \n",
    "latitude = data['results'][0]['geometry']['location']['lat'] \n",
    "longitude = data['results'][0]['geometry']['location']['lng'] \n",
    "formatted_address = data['results'][0]['formatted_address'] \n",
    "  \n",
    "# printing the output \n",
    "print(\"Latitude:%s\\nLongitude:%s\\nFormatted Address:%s\"\n",
    "      %(latitude, longitude,formatted_address)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'username': 'xxxxxhh', 'password': 'password'}\n",
    "r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)\n",
    "print(r.cookies.get_dict())\n",
    "\n",
    "# {'username': 'Morvan', 'loggedin': '1'}\n",
    "\n",
    "\n",
    "r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', cookies=r.cookies)\n",
    "print(r.text)\n",
    "\n",
    "# Hey Morvan! Looks like you're still logged into the site!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', cookies=r.cookies)\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "payload = {'username': 'Morvanxxx', 'password': 'password'}\n",
    "r = session.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)\n",
    "print(r.cookies.get_dict())\n",
    "\n",
    "# {'username': 'Morvan', 'loggedin': '1'}\n",
    "\n",
    "\n",
    "r = session.get(\"http://pythonscraping.com/pages/cookies/profile.php\")\n",
    "print(r.text)\n",
    "\n",
    "# Hey Morvan! Looks like you're still logged into the site!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loggedin': '1', 'username': 'Morvan'}\n",
      "\n",
      "<h2>Welcome to the Website!</h2>\n",
      "You have logged in successfully! <br><a href=\"profile.php\">Check out your profile!</a>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "payload = {'username':'Morvan', 'password':'password'}\n",
    "r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)\n",
    "print(r.cookies.get_dict())\n",
    "# 此处链接更新，测试OK\n",
    "r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', cookies=r.cookies)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loggedin': '1', 'username': 'Morvanxxx'}\n",
      "\n",
      "<h2>Welcome to the Website!</h2>\n",
      "You have logged in successfully! <br><a href=\"profile.php\">Check out your profile!</a>\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "payload = {'username': 'Morvanxxx', 'password': 'password'}\n",
    "r = session.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)\n",
    "print(r.cookies.get_dict())\n",
    "\n",
    "# {'username': 'Morvan', 'loggedin': '1'}\n",
    "\n",
    "\n",
    "r = session.get(\"http://pythonscraping.com/pages/cookies/welcome.php\")\n",
    "print(r.text)\n",
    "\n",
    "# Hey Morvan! Looks like you're still logged into the site!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./img/', exist_ok=True)\n",
    "\n",
    "IMAGE_URL = \"https://morvanzhou.github.io/static/img/description/learning_step_flowchart.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./img/image1.png', <http.client.HTTPMessage at 0x1681765af40>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve(IMAGE_URL, './img/image1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(IMAGE_URL)\n",
    "with open('./img/image2.png', 'wb') as f:\n",
    "    f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20200213043803703.jpg\n",
      "Saved thumb_469_352_20200103053844300.jpg\n",
      "Saved thumb_469_352_20191231044318886.jpg\n",
      "Saved thumb_469_352_20191226042614188.jpg\n",
      "Saved thumb_469_352_20191225101456715.jpg\n",
      "Saved 20171024023609391.jpg\n",
      "Saved thumb_469_352_20180824054325356.jpg\n",
      "Saved thumb_469_352_20180823054126855.jpg\n",
      "Saved thumb_469_352_20180717055303642.jpg\n",
      "Saved thumb_469_352_20180716055214274.jpg\n",
      "Saved 20200219020544363.jpg\n",
      "Saved thumb_469_352_20200218030404136.jpg\n",
      "Saved thumb_469_352_20200213100226980.jpg\n",
      "Saved thumb_469_352_20200212044113654.jpg\n",
      "Saved thumb_469_352_20200204011559621.jpg\n",
      "Saved 20200220022650769.jpg\n",
      "Saved thumb_469_352_20200212015817646.jpg\n",
      "Saved thumb_469_352_20200210105022549.jpg\n",
      "Saved thumb_469_352_20200207101427298.jpg\n",
      "Saved thumb_469_352_20200206012705629.jpg\n",
      "Saved 20200218061446407.jpg\n",
      "Saved thumb_469_352_20200110101443871.jpg\n",
      "Saved thumb_469_352_20200108100432769.jpg\n",
      "Saved thumb_469_352_20200107042058654.jpg\n",
      "Saved thumb_469_352_20200102055832512.jpg\n",
      "Saved 20200221041535191.jpg\n",
      "Saved thumb_469_352_20200221014755136.jpg\n",
      "Saved thumb_469_352_20200220051520906.jpg\n",
      "Saved thumb_469_352_20200217045514592.jpg\n",
      "Saved thumb_469_352_20200214025945668.jpg\n",
      "Saved 20200214041407506.jpg\n",
      "Saved thumb_469_352_20200211044139532.jpg\n",
      "Saved thumb_469_352_20200210042037406.jpg\n",
      "Saved thumb_469_352_20200203061947619.jpg\n",
      "Saved thumb_469_352_20200115103021627.jpg\n",
      "Saved 20171127110451260.jpg\n",
      "Saved 20171127110544594.jpg\n",
      "Saved 20171127110817403.jpg\n",
      "Saved 20200219043141640.jpg\n",
      "Saved 20200219020544363.jpg\n",
      "Saved 20200218030404136.jpg\n",
      "Saved 20200218105340160.jpg\n",
      "Saved 20200213100226980.jpg\n",
      "Saved 20200212044113654.jpg\n",
      "Saved 20171127111001137.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.request import urlretrieve\n",
    "url = \"http://www.ngchina.com.cn/animals/\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "# img_ul = soup.find_all('ul',{\"class\":\"img_list\"})\n",
    "img_ul = soup.find_all(\"img\",{\"src\": re.compile(\"^http://image.ngchina.com.cn/.*\\.jpg$\")})\n",
    "for img_src in img_ul:\n",
    "#     img_url = img_src.img[\"src\"]\n",
    "    img_url = img_src[\"src\"]\n",
    "#     path = \"./img/image\"+str(count)+\".jpg\"\n",
    "#     urlretrieve(img_url,path)\n",
    "    img_name = img_url.split(\"/\")[-1]\n",
    "#     path = \"./img/\"+img_name\n",
    "    urlretrieve(img_url,\"./img/%s\"%(img_name))\n",
    "    print(\"Saved %s\"%(img_name))\n",
    "# img_src = img_ul.find(\"img\",{\"src\": re.compile(\"^http://image.ngchina.com.cn/.*\\.jpg$\")})\n",
    "\n",
    "# \n",
    "# for i in img_link:\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://www.nationalgeographic.com.cn/animals/\"\n",
    "html = requests.get(URL).text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "img_ul = soup.find_all('ul', {\"class\": \"img_list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20200213043803703.jpg\n",
      "Saved thumb_469_352_20200103053844300.jpg\n",
      "Saved thumb_469_352_20191231044318886.jpg\n",
      "Saved thumb_469_352_20191226042614188.jpg\n",
      "Saved thumb_469_352_20191225101456715.jpg\n",
      "Saved 20171024023609391.jpg\n",
      "Saved thumb_469_352_20180824054325356.jpg\n",
      "Saved thumb_469_352_20180823054126855.jpg\n",
      "Saved thumb_469_352_20180717055303642.jpg\n",
      "Saved thumb_469_352_20180716055214274.jpg\n",
      "Saved 20200219020544363.jpg\n",
      "Saved thumb_469_352_20200218030404136.jpg\n",
      "Saved thumb_469_352_20200213100226980.jpg\n",
      "Saved thumb_469_352_20200212044113654.jpg\n",
      "Saved thumb_469_352_20200204011559621.jpg\n",
      "Saved 20200220022650769.jpg\n",
      "Saved thumb_469_352_20200212015817646.jpg\n",
      "Saved thumb_469_352_20200210105022549.jpg\n",
      "Saved thumb_469_352_20200207101427298.jpg\n",
      "Saved thumb_469_352_20200206012705629.jpg\n",
      "Saved 20200218061446407.jpg\n",
      "Saved thumb_469_352_20200110101443871.jpg\n",
      "Saved thumb_469_352_20200108100432769.jpg\n",
      "Saved thumb_469_352_20200107042058654.jpg\n",
      "Saved thumb_469_352_20200102055832512.jpg\n",
      "Saved 20200221041535191.jpg\n",
      "Saved thumb_469_352_20200221014755136.jpg\n",
      "Saved thumb_469_352_20200220051520906.jpg\n",
      "Saved thumb_469_352_20200217045514592.jpg\n",
      "Saved thumb_469_352_20200214025945668.jpg\n",
      "Saved 20200214041407506.jpg\n",
      "Saved thumb_469_352_20200211044139532.jpg\n",
      "Saved thumb_469_352_20200210042037406.jpg\n",
      "Saved thumb_469_352_20200203061947619.jpg\n",
      "Saved thumb_469_352_20200115103021627.jpg\n",
      "Saved 20171127110451260.jpg\n",
      "Saved 20171127110544594.jpg\n",
      "Saved 20171127110817403.jpg\n",
      "Saved 20200219043141640.jpg\n",
      "Saved 20200219020544363.jpg\n",
      "Saved 20200218030404136.jpg\n",
      "Saved 20200218105340160.jpg\n",
      "Saved 20200213100226980.jpg\n",
      "Saved 20200212044113654.jpg\n",
      "Saved 20171127111001137.jpg\n"
     ]
    }
   ],
   "source": [
    "for img in img_ul:\n",
    "    img_url = img[\"src\"]\n",
    "    r = requests.get(img_url,stream=True)\n",
    "    img = r.iter_content(chunk_size=128)\n",
    "    img_name = img_url.split('/')[-1]\n",
    "#     path = \"./img/\"+img_name\n",
    "    with open(\"./img/%s\"%(img_name),\"wb\") as f:\n",
    "        for i in img:\n",
    "            f.write(i)\n",
    "    print(\"Saved %s\"%(img_name))\n",
    "# img_ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "from urllib.request import urlopen, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# base_url = \"http://127.0.0.1:4000/\"\n",
    "base_url = 'https://morvanzhou.github.io/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    response = urlopen(url)\n",
    "#     time.sleep(0.1)             # slightly delay for downloading\n",
    "    print(url)\n",
    "    return response.read().decode()\n",
    "\n",
    "\n",
    "def parse(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')})\n",
    "    title = soup.find('h1').get_text().strip()\n",
    "    page_urls = set([urljoin(base_url, url['href']) for url in urls])   # 去重\n",
    "    url = soup.find('meta', {'property': \"og:url\"})['content']\n",
    "    return title, page_urls, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = set([base_url,])\n",
    "seen = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# while len(unseen) != 0:\n",
    "#     if len(seen)>=20:\n",
    "#         break\n",
    "#     htmls = [crawl(url) for url in unseen]\n",
    "#     results = [parse(html) for html in htmls]\n",
    "#     seen.update(unseen)\n",
    "#     unseen.clear()\n",
    "#     count += 1\n",
    "#     print(\"第%d遍\" % (count))\n",
    "\n",
    "#     for title, page_urls, url in results:\n",
    "#         unseen.update(page_urls - seen)\n",
    "#         print(title, page_urls, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributed Crawling...\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "from urllib.request import urlopen, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# base_url = \"http://127.0.0.1:4000/\"\n",
    "base_url = 'https://morvanzhou.github.io/'\n",
    "\n",
    "def crawl(url):\n",
    "    response = urlopen(url)\n",
    "#     time.sleep(0.1)             # slightly delay for downloading\n",
    "    print(url)\n",
    "    return response.read().decode()\n",
    "\n",
    "\n",
    "def parse(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')})\n",
    "    title = soup.find('h1').get_text().strip()\n",
    "    page_urls = set([urljoin(base_url, url['href']) for url in urls])   # 去重\n",
    "    url = soup.find('meta', {'property': \"og:url\"})['content']\n",
    "    return title, page_urls, url\n",
    "\n",
    "unseen = set([base_url,])\n",
    "seen = set()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool()\n",
    "    count, t1 = 1, time.time()\n",
    "    while len(unseen) != 0:\n",
    "        if len(seen) >= 20:\n",
    "            break\n",
    "        print('\\nDistributed Crawling...')\n",
    "        crawl_jobs = [pool.apply_async(crawl, args=(url,)) for url in unseen]\n",
    "        htmls = [j.get() for j in crawl_jobs]\n",
    "\n",
    "        print('\\nDistributed Crawling...')\n",
    "        parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls]\n",
    "        results = [k.get() for k in parse_jobs]\n",
    "\n",
    "        print('\\nDistributed Crawling...')\n",
    "        seen.update(unseen)\n",
    "        unseen.clear()\n",
    "\n",
    "        print(\"第%d遍\" % (count))\n",
    "\n",
    "        for title, page_urls, url in results:\n",
    "            unseen.update(page_urls - seen)\n",
    "            print(count, title, url)\n",
    "            count += 1\n",
    "    print('Total time: %.1f s' % (time.time() - t1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
